{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as FT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Sample of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/jaero/interviews/aws/vector_search/data/\"\n",
    "GEO_DATA_DIR = os.path.join(DATA_DIR, \"geological_similarity\")\n",
    "classes = sorted([c for c in os.listdir(GEO_DATA_DIR) if c != \".DS_Store\"])\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_as_np_array(fpath: str) -> np.ndarray:\n",
    "    \"\"\"Utility function to read in image file as numpy array\n",
    "\n",
    "    Args:\n",
    "        fpath (str): file path to image\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: image returned as numpy array\n",
    "    \"\"\"\n",
    "    with Image.open(fpath) as im:\n",
    "        arr = np.asarray(im.convert('RGB'))\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = os.path.join(GEO_DATA_DIR, \"gneiss/0A3V2.jpg\")\n",
    "arr = read_image_as_np_array(sample_file)\n",
    "print(arr.shape)\n",
    "Image.open(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- Dimensions of image are very small\n",
    "- low resolution may make it difficult to retrieve features from to distinguish from other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showcase an example of each class\n",
    "# use some basic upsampling for visualization purposes\n",
    "import random\n",
    "\n",
    "image_dict = {}\n",
    "for c in classes:\n",
    "    dirpath = os.path.join(GEO_DATA_DIR, c)\n",
    "    sample_path = os.path.join(dirpath, os.listdir(dirpath)[random.randint(0, len(os.listdir(dirpath)))])\n",
    "    arr = read_image_as_np_array(sample_path)\n",
    "    print(f\"\\n{sample_path}\")\n",
    "    print(f\"\\t{c} - Before Upsampling: \", arr.shape)\n",
    "    tensor = torch.tensor(arr).unsqueeze(0).permute(0,3,1,2)\n",
    "    upsample_layer = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "    tensor = upsample_layer(tensor)\n",
    "    tensor = tensor.squeeze(0).permute(1,2,0)\n",
    "    print(f\"\\t{c} - After Upsampling: \", tensor.shape)\n",
    "    img = Image.fromarray(tensor.detach().cpu().numpy())\n",
    "    image_dict[c] = img\n",
    "\n",
    "\n",
    "for c, img in image_dict.items():\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(c)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- upsampling to 56x56 helps to visualize images, but apart from color and some dense pixel areas the clarity and edge feautures are a bit obscure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata of Provided Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for c in classes:\n",
    "    sizes = set()\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(os.path.join(GEO_DATA_DIR, c)):\n",
    "        for file in files:\n",
    "            fpath = os.path.join(root, file)\n",
    "            arr = read_image_as_np_array(fpath)\n",
    "            shape = arr.shape\n",
    "            sizes.add(shape)\n",
    "            count += 1 \n",
    "    metadata.append({'class': c, 'sizes': [s for s in sizes], 'count': count})\n",
    "\n",
    "df = pd.DataFrame(metadata)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- consistent image sizes\n",
    "- class distribution seem to be fairly balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt Super Resolution Using Pretrained Model ESRGAN\n",
    "- Source Attribution: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code in this block was taken from: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Super-Resolution\n",
    "in order to initialize a pretrained Super Resolution Model for input image augmentation\n",
    "\n",
    "License: MIT License\n",
    "\"\"\"\n",
    "\n",
    "# Constants\n",
    "rgb_weights = torch.FloatTensor([65.481, 128.553, 24.966]).to(device)\n",
    "imagenet_mean = torch.FloatTensor([0.485, 0.456, 0.406]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_std = torch.FloatTensor([0.229, 0.224, 0.225]).unsqueeze(1).unsqueeze(2)\n",
    "imagenet_mean_cuda = torch.FloatTensor([0.485, 0.456, 0.406]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "imagenet_std_cuda = torch.FloatTensor([0.229, 0.224, 0.225]).to(device).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block, comprising convolutional, BN, activation layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        \"\"\"\n",
    "        :param in_channels: number of input channels\n",
    "        :param out_channels: number of output channe;s\n",
    "        :param kernel_size: kernel size\n",
    "        :param stride: stride\n",
    "        :param batch_norm: include a BN layer?\n",
    "        :param activation: Type of activation; None if none\n",
    "        \"\"\"\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        # A container that will hold the layers in this convolutional block\n",
    "        layers = list()\n",
    "\n",
    "        # A convolutional layer\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2))\n",
    "\n",
    "        # A batch normalization (BN) layer, if wanted\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        # An activation layer, if wanted\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        # Put together the convolutional block as a sequence of the layers in this container\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, in_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, out_channels, w, h)\n",
    "        \"\"\"\n",
    "        output = self.conv_block(input)  # (N, out_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A subpixel convolutional block, comprising convolutional, pixel-shuffle, and PReLU activation layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size of the convolution\n",
    "        :param n_channels: number of input and output channels\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions)\n",
    "        \"\"\"\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "\n",
    "        # A convolutional layer that increases the number of channels by scaling factor^2, followed by pixel shuffle and PReLU\n",
    "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        # These additional channels are shuffled to form additional pixels, upscaling each dimension by the scaling factor\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: scaled output images, a tensor of size (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv(input)  # (N, n_channels * scaling factor^2, w, h)\n",
    "        output = self.pixel_shuffle(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        output = self.prelu(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block, comprising two convolutional blocks with a residual connection across them.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=3, n_channels=64):\n",
    "        \"\"\"\n",
    "        :param kernel_size: kernel size\n",
    "        :param n_channels: number of input and output channels (same because the input must be added to the output)\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # The second convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        :param input: input images, a tensor of size (N, n_channels, w, h)\n",
    "        :return: output images, a tensor of size (N, n_channels, w, h)\n",
    "        \"\"\"\n",
    "        residual = input  # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The SRResNet, as defined in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # Scaling factor must be 2, 4, or 8\n",
    "        scaling_factor = int(scaling_factor)\n",
    "        assert scaling_factor in {2, 4, 8}, \"The scaling factor must be 2, 4, or 8!\"\n",
    "\n",
    "        # The first convolutional block\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='PReLu')\n",
    "\n",
    "        # A sequence of n_blocks residual blocks, each containing a skip-connection across the block\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for i in range(n_blocks)])\n",
    "\n",
    "        # Another convolutional block\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
    "                                              kernel_size=small_kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "        # Upscaling is done by sub-pixel convolution, with each such block upscaling by a factor of 2\n",
    "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for i\n",
    "              in range(n_subpixel_convolution_blocks)])\n",
    "\n",
    "        # The last convolutional block\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='Tanh')\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n",
    "        residual = output  # (N, n_channels, w, h)\n",
    "        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "        sr_imgs = self.conv_block3(output)  # (N, 3, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return sr_imgs\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator in the SRGAN, as defined in the paper. Architecture identical to the SRResNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        :param large_kernel_size: kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "        :param small_kernel_size: kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "        :param n_channels: number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "        :param n_blocks: number of residual blocks\n",
    "        :param scaling_factor: factor to scale input images by (along both dimensions) in the subpixel convolutional block\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # The generator is simply an SRResNet, as above\n",
    "        self.net = SRResNet(large_kernel_size=large_kernel_size, small_kernel_size=small_kernel_size,\n",
    "                            n_channels=n_channels, n_blocks=n_blocks, scaling_factor=scaling_factor)\n",
    "\n",
    "    def initialize_with_srresnet(self, srresnet_checkpoint):\n",
    "        \"\"\"\n",
    "        Initialize with weights from a trained SRResNet.\n",
    "\n",
    "        :param srresnet_checkpoint: checkpoint filepath\n",
    "        \"\"\"\n",
    "        srresnet = torch.load(srresnet_checkpoint)['model']\n",
    "        self.net.load_state_dict(srresnet.state_dict())\n",
    "\n",
    "        print(\"\\nLoaded weights from pre-trained SRResNet.\\n\")\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        Forward prop.\n",
    "\n",
    "        :param lr_imgs: low-resolution input images, a tensor of size (N, 3, w, h)\n",
    "        :return: super-resolution output images, a tensor of size (N, 3, w * scaling factor, h * scaling factor)\n",
    "        \"\"\"\n",
    "        sr_imgs = self.net(lr_imgs)  # (N, n_channels, w * scaling factor, h * scaling factor)\n",
    "\n",
    "        return sr_imgs\n",
    "\n",
    "\n",
    "## Utility Classes/Functions\n",
    "\n",
    "def convert_image(img, source, target):\n",
    "    \"\"\"\n",
    "    Convert an image from a source format to a target format.\n",
    "\n",
    "    :param img: image\n",
    "    :param source: source format, one of 'pil' (PIL image), '[0, 1]' or '[-1, 1]' (pixel value ranges)\n",
    "    :param target: target format, one of 'pil' (PIL image), '[0, 255]', '[0, 1]', '[-1, 1]' (pixel value ranges),\n",
    "                   'imagenet-norm' (pixel values standardized by imagenet mean and std.),\n",
    "                   'y-channel' (luminance channel Y in the YCbCr color format, used to calculate PSNR and SSIM)\n",
    "    :return: converted image\n",
    "    \"\"\"\n",
    "    assert source in {'pil', '[0, 1]', '[-1, 1]'}, \"Cannot convert from source format %s!\" % source\n",
    "    assert target in {'pil', '[0, 255]', '[0, 1]', '[-1, 1]', 'imagenet-norm',\n",
    "                      'y-channel'}, \"Cannot convert to target format %s!\" % target\n",
    "\n",
    "    # Convert from source to [0, 1]\n",
    "    if source == 'pil':\n",
    "        img = FT.to_tensor(img)\n",
    "\n",
    "    elif source == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif source == '[-1, 1]':\n",
    "        img = (img + 1.) / 2.\n",
    "\n",
    "    # Convert from [0, 1] to target\n",
    "    if target == 'pil':\n",
    "        img = FT.to_pil_image(img)\n",
    "\n",
    "    elif target == '[0, 255]':\n",
    "        img = 255. * img\n",
    "\n",
    "    elif target == '[0, 1]':\n",
    "        pass  # already in [0, 1]\n",
    "\n",
    "    elif target == '[-1, 1]':\n",
    "        img = 2. * img - 1.\n",
    "\n",
    "    elif target == 'imagenet-norm':\n",
    "        if img.ndimension() == 3:\n",
    "            img = (img - imagenet_mean) / imagenet_std\n",
    "        elif img.ndimension() == 4:\n",
    "            img = (img - imagenet_mean_cuda) / imagenet_std_cuda\n",
    "\n",
    "    elif target == 'y-channel':\n",
    "        # Based on definitions at https://github.com/xinntao/BasicSR/wiki/Color-conversion-in-SR\n",
    "        # torch.dot() does not work the same way as numpy.dot()\n",
    "        # So, use torch.matmul() to find the dot product between the last dimension of an 4-D tensor and a 1-D tensor\n",
    "        img = torch.matmul(255. * img.permute(0, 2, 3, 1)[:, 4:-4, 4:-4, :], rgb_weights) / 255. + 16.\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "srresnet = SRResNet()\n",
    "srgan_generator = Generator()\n",
    "\n",
    "# Model checkpoints\n",
    "srgan_checkpoint = \"../saved_models/checkpoint_srgan.pth.tar\"\n",
    "srresnet_checkpoint = \"../saved_models/checkpoint_srresnet.pth.tar\"\n",
    "\n",
    "# Load models\n",
    "srresnet.load_state_dict(torch.load(srresnet_checkpoint,  weights_only=True))\n",
    "srgan_generator.load_state_dict(torch.load(srgan_checkpoint,  weights_only=True))\n",
    "\n",
    "srresnet.to(device)\n",
    "srgan_generator.to(device)\n",
    "\n",
    "srresnet.eval()\n",
    "srgan_generator.eval()\n",
    "\n",
    "\n",
    "def test_super_res(img_path):\n",
    "    lr_img = Image.open(img_path).convert('RGB')\n",
    "    bicubic_img  = lr_img.resize((112, 112), Image.BICUBIC)\n",
    "    hr_img = bicubic_img\n",
    "\n",
    "    # Super-resolution (SR) with SRResNet\n",
    "    sr_img_srresnet = srresnet(convert_image(lr_img, source='pil', target='imagenet-norm').unsqueeze(0).to(device))\n",
    "    sr_img_srresnet = sr_img_srresnet.squeeze(0).cpu().detach()\n",
    "    sr_img_srresnet = convert_image(sr_img_srresnet, source='[-1, 1]', target='pil')\n",
    "\n",
    "    # Super-resolution (SR) with SRGAN\n",
    "    sr_img_srgan = srgan_generator(convert_image(lr_img, source='pil', target='imagenet-norm').unsqueeze(0).to(device))\n",
    "    sr_img_srgan = sr_img_srgan.squeeze(0).cpu().detach()\n",
    "    sr_img_srgan = convert_image(sr_img_srgan, source='[-1, 1]', target='pil')\n",
    "\n",
    "    return lr_img, bicubic_img, sr_img_srresnet, sr_img_srgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showcase an example of each class\n",
    "# use some basic upsampling for visualization purposes\n",
    "image_dict = {}\n",
    "\n",
    "for c in classes:\n",
    "    # read in image from file\n",
    "    dirpath = os.path.join(GEO_DATA_DIR, c)\n",
    "    sample_path = os.path.join(dirpath, os.listdir(dirpath)[random.randint(0, len(os.listdir(dirpath)))])\n",
    "    print(sample_path)\n",
    "    lr_img, bicubic_img, sr_img_srresnet, sr_img_srgan = test_super_res(sample_path)\n",
    "    print(f'\\tSuper Res Dimensions: {sr_img_srgan.size}')\n",
    "    plot_data = [(f'{c}-original', lr_img), \n",
    "                 (f'{c}-bicubic', bicubic_img), \n",
    "                 (f'{c}-srresnet', sr_img_srresnet), \n",
    "                 (f'{c}-srgan', sr_img_srgan)]\n",
    "    for title, img in plot_data:\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resolution quality is a bit better, though GAN seems to have smoothed the edges and provide more distinctive features, so may prefer to use SRGAN instead of SRResnet\n",
    "- [Future Work] May be worth training a custom low res -> high res model for these specific geo images\n",
    "- (112, 112) dim should be a large enough dimension to work with for encoding image into image embeddings for vector similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it Possible to Train a Simple, Performant Image Classifcation on this Data? \n",
    "- Should help determine how useful our reduced dimensionality vector similarity search engine will be\n",
    "- Especially when it comes to distinction of features across classes and similarity of features within each class\n",
    "- Q: Will super resolution improve the models performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test Split\n",
    "- Can be used for classification as well as image embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, Test Split\n",
    "random.seed(10)\n",
    "\n",
    "data = []\n",
    "for c in classes:\n",
    "    for root, dirs, files in os.walk(os.path.join(GEO_DATA_DIR, c)):\n",
    "        for file in files:\n",
    "            fpath = os.path.join(root, file)\n",
    "            img = Image.open(fpath).convert('RGB')\n",
    "            tensor = FT.to_tensor(img)\n",
    "            mean, std = tensor.mean(), tensor.std()\n",
    "            data.append({ 'class': c, 'file': file, 'file_path': fpath, 'mean': mean.numpy(), 'std': std.numpy() })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['train'] = 0\n",
    "df['val'] = 0\n",
    "df['test'] = 0\n",
    "\n",
    "# shuffle and split across classes for balancing purposes\n",
    "# 60:20:20 train:val:test\n",
    "train, val ,test = [], [], []\n",
    "for c in classes:\n",
    "    file_paths = df[df['class'].eq(c)]['file_path'].to_list()\n",
    "    random.shuffle(file_paths)\n",
    "    N = len(file_paths)\n",
    "    train_idx = round(N*0.6)\n",
    "    val_idx = train_idx + round(N*0.2)\n",
    "    test_idx = val_idx + round(N*0.2)\n",
    "    if test_idx > N: test_idx = N\n",
    "    # split\n",
    "    train.extend(file_paths[:train_idx])\n",
    "    val.extend(file_paths[train_idx:val_idx])\n",
    "    test.extend(file_paths[val_idx:test_idx])\n",
    "\n",
    "train = {f: None for f in train}\n",
    "val = {f: None for f in val}\n",
    "test = {f: None for f in test}\n",
    "\n",
    "# Update df\n",
    "for i, row in df.iterrows():\n",
    "    fpath = row['file_path']\n",
    "    if fpath in train: df.at[i, 'train'] = 1\n",
    "    elif fpath in val: df.at[i, 'val'] = 1\n",
    "    elif fpath in test: df.at[i, 'test'] = 1\n",
    "\n",
    "\n",
    "train_count = df['train'].value_counts()[1]\n",
    "val_count = df['val'].value_counts()[1]\n",
    "test_count = df['test'].value_counts()[1]\n",
    "\n",
    "print(f\"Total Samples: {len(df.index)}\")\n",
    "print(train_count, val_count, test_count)\n",
    "assert len(df.index) == train_count + val_count + test_count\n",
    "\n",
    "print(f\"\\nTRAIN\\n{df[df['train'].eq(1)]['class'].value_counts()}\\n\")\n",
    "print(f\"VALIDATION\\n{df[df['val'].eq(1)]['class'].value_counts()}\\n\")\n",
    "print(f\"TEST\\n{df[df['test'].eq(1)]['class'].value_counts()}\\n\")\n",
    "\n",
    "# save df split\n",
    "df.to_csv(os.path.join(DATA_DIR, 'train_val_test_splits', 'data_split-1.csv'), index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Search - Solutioning\n",
    "\n",
    "- **Step 1:** SuperResolutionModel(OriginalImage (24x24)) --> UsableImage (112x112)\n",
    "- **Step 2:** ImageEmbeddingModel(UsableImage) -> VectorEmbedding () [requires model training for ImageEmbeddingModel]\n",
    "- **Step 3:** Create Vector Index for Approximate Nearest Neighbor Search (e.g. FAISS, ANNOY) \n",
    "- **Step 3a:** (Optional) Utilize a Vector Database (e.g. PineconeDB) to store image embeddings\n",
    "- **Step 4:** Build and Deploy Containerized Web Application for users to be able to upload image of their choice and return Top K similar images\n",
    "- **Step 4a:** Top K+N similar vector candidates returned using direct similarity metric calculation (e.g. cosine similarity) where N is tunable depending on computational vs accuracy performance balance\n",
    "- **Step 4b:** (Optional) Similar vector candidates are then re-ranked according to similarity based on original resolution of images instead in order to account for potential information loss when encoding images into smaller vector representations\n",
    "- **Step 4c:** Top K similar images are presented to the user based on the input image they uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
